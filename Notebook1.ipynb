{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d36fJduDL40h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwQ0aK6hL5wQ"
   },
   "source": [
    "# Set up\n",
    "The first thing that we need to do is to import all the libraries that we are going to use.\n",
    "For this first Notebook we need:\n",
    "\n",
    "- **os**: The os module provides a way of using operating system dependent functionality e.g. importing files or changing directories\n",
    "- **re**: The re module provides support for regular expressions e.g. searching, matching, and splitting the string according to specified patterns.\n",
    "- **pandas**: The pandas module provides support to work with datasets e.g. data cleaning andand transformation\n",
    "- **nltk**: The NLTK library allows you to work with human language data (natural language processing). It includes libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning\n",
    "- **numpy**: The numpy module is for scientific computing with Python. This will help with performing linear algebra and mathematic operations on your dataset\n",
    "- **codecs**: The codecs module helps dealing with encoding/decoding, especially Unicode\n",
    "- **matplotlib**: The matplotlib module will allow you to create visualisations\n",
    "- **seaborn**: The matplotlib module will allow you to create visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-V06TiHMHAd",
    "outputId": "dbe82914-93eb-4a6a-82ed-d5806a33128b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd # We are renaming pandas as pd to be faster in calling it\n",
    "import nltk\n",
    "nltk.download('punkt_tab')# within nltk we need to download or import a series of submodules. Punkt is used for tokenizing sentences\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize # This is a specific import from the nltk.tokenize module, bringing in the word_tokenize function, which is used to split text into words.\n",
    "from nltk.tokenize import sent_tokenize # Import the tokeniser by sentences\n",
    "from nltk.corpus import stopwords # this import the stopwords\n",
    "import numpy as np\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a1GdaEvOblc"
   },
   "source": [
    "# Our Dataset\n",
    "The Statistical Accounts of Scotland are a series of documentary publications, related in subject matter though published at different times, covering life in Scotland in the 18th and 19th.\n",
    "\n",
    "The Old (or First) Statistical Account of Scotland was published between 1791 and 1799 by Sir John Sinclair of Ulbster. The New (or Second) Statistical Account of Scotland published under the auspices of the General Assembly of the Church of Scotland between 1834 and 1845. These first two Statistical Accounts of Scotland are unique records of life during the agricultural and industrial revolutions in Europe.\n",
    "\n",
    "## Structure of the dataset\n",
    "The original publication has been scanned and OCRed and each single record has been collected in a .txt file. The name of each file contain information about the document itself. For example StAS.2.15.91.P.Orkney.Cross_and_Burness\n",
    "\n",
    "- StAs.2.15.91 -> Second Statistical Account\n",
    "- P -> Parish (Contain information from the Parish)\n",
    "\n",
    "- Orkney -> Area of interest (Scotland has been divided in 33 Areas)\n",
    "\n",
    "- Cross_and_Burness -> Parish\n",
    "\n",
    "We are going to see how to use this to extract information about all our text later but the first thing we need to do is to create a single dataframe (table) that will contain all the texts otherwise it will be very difficult to manage the data.\n",
    "\n",
    "## Prepare the dataset\n",
    "All our .txt files are in a directory named Account so I can write a function that will loop through each of the files extract the text and the tile of each file and put them all in a table.\n",
    "\n",
    "Doing it manually would take a ridiculous amount of time but that is what computer are for so let’s see what we can do.\n",
    "\n",
    "### 1. Import the data from the StatAccount repo\n",
    "Our data are hosted on a separate repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpKLn1zMOiZH",
    "outputId": "ff042483-f093-47f2-9528-d619b0aab484"
   },
   "outputs": [],
   "source": [
    " !git clone https://github.com/DCS-training/StatAccountScotland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check on left handside can you see a StatAccountScotland folder?\n",
    "\n",
    "### 2. Import text and title of each .txt file\n",
    "The next step would be to create a for loop that will run through each file and import the text and title into a two column dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mytyek1PUf70"
   },
   "outputs": [],
   "source": [
    "# Create a string object that contains the path to your directory\n",
    "text_files_dir = \"StatAccountScotland/Accounts\"\n",
    "\n",
    "# Create an empty DataFrame.\n",
    "text_data = pd.DataFrame(columns=['title', 'text'])\n",
    "\n",
    "# Get a list of all .txt files in the specified directory\n",
    "text_files = [os.path.join(text_files_dir, f) for f in os.listdir(text_files_dir) if f.endswith('.txt')]\n",
    "\n",
    "# Iterate through each text file\n",
    "for file_path in text_files:\n",
    "    # Open the file with codecs to handle the encoding\n",
    "    with codecs.open(file_path, 'r', encoding='latin1') as file:\n",
    "        # Read lines, convert to lowercase and join them into a single string\n",
    "        text = ' '.join([line.strip().lower() for line in file])\n",
    "\n",
    "    # Extract the title from the filename (remove path and file extension)\n",
    "    title = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Create a new DataFrame for the current file and append it to the main DataFrame using pd.concat\n",
    "    new_row = pd.DataFrame({'title': [title], 'text': [text]})\n",
    "    text_data = pd.concat([text_data, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "wQqiAjfuVcd8",
    "outputId": "4f4fe92f-ff51-4770-8c86-18fbe4dd65d5"
   },
   "outputs": [],
   "source": [
    "#Look at the first five rows\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "o52PdjOAVilC",
    "outputId": "d151e955-afeb-4528-f495-fd90b8dabf1b"
   },
   "outputs": [],
   "source": [
    "#Look at the last five rows\n",
    "text_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "_wN5RfWIVl0W",
    "outputId": "0e6a0997-91cb-4682-da05-aff851ef0992"
   },
   "outputs": [],
   "source": [
    "# Getting descriptive statistics\n",
    "text_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6CyIxy0_Tq7"
   },
   "outputs": [],
   "source": [
    "text_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eP5zL2dpVyoZ"
   },
   "outputs": [],
   "source": [
    "# We can also save our new dataframe as a.csv file so that we will be easily re-import it\n",
    "text_data.to_csv('text_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXzTuQMvXsY5"
   },
   "source": [
    "#### Exercise 1:\n",
    "\n",
    "The code below allows you to explore specific cells of your dataset\n",
    "- 100  is the row number\n",
    "- 1 is the column number\n",
    "\n",
    "NB: Remember that Python start counting from 0 so if you want the second row of the secon column you need to use [1,1]\n",
    "\n",
    "- What can you tell about the dataset?\n",
    "- Can you identify any issue/further data clean that we need to do?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wH200Ey_XmEo",
    "outputId": "10bac07d-2cc3-4b84-973d-5142af15bde2"
   },
   "outputs": [],
   "source": [
    "print(text_data.iloc[100, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra4byKcHYlWQ"
   },
   "source": [
    "## Clean and Format the Data\n",
    "Our dataset contains all the info that we want but there are still some formatting issues and informations that are all crammed in a single cell. Here is where Regex can come handy!\n",
    "\n",
    "### 1. Removing the sequences of a dash followed by one or more spaces\n",
    "This will help us fixing the going to solve the split across multiple rows word issue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j14LIEvYw4A"
   },
   "outputs": [],
   "source": [
    "# Define a function to replace patterns in string columns\n",
    "def replace_pattern(text):\n",
    "    if isinstance(text, str):\n",
    "        # Use re.sub() to replace the pattern '-[space]' followed by one or more spaces\n",
    "        return re.sub(r'-\\s+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UKFDpUPYzHR"
   },
   "outputs": [],
   "source": [
    "ScotdataClean = text_data.map(replace_pattern) #map the function across the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pp2wSwnwZcAd",
    "outputId": "b268cab7-2528-489b-a7d5-f8ad119c7c6b"
   },
   "outputs": [],
   "source": [
    "#Check the result\n",
    "print(ScotdataClean.iloc[100, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Bvwv6OPZY-2"
   },
   "source": [
    "### 2 Extract more info from the Dataset\n",
    "\n",
    "To do the following steps we are using regex. Short for regular expression, a regex is a string of text that lets you create patterns that help match, locate, and manage text. Think find and replace in Word.\n",
    "As we have seen in the slides building a regular expression that works can take some time and it is a good idea to use\n",
    "\n",
    "#### 2.1 Extract area and parish from the title\n",
    "P=Parish\n",
    "\n",
    "*  P=Parish\n",
    "*  C=Miscellanea\n",
    "*  G=General Observations\n",
    "*  A=Appendix\n",
    "*  F=General\n",
    "*  I=Index\n",
    "*  M=Map\n",
    "\n",
    "I want to be able to subset the dataset by those and I also want to have them both as a code as a description to do so I need to write a if else clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Wvba6bidZ-Nq",
    "outputId": "e97a99c4-8455-4866-ce09-dbb2cea987bf"
   },
   "outputs": [],
   "source": [
    "# Extract single characters as specified by the regex pattern\n",
    "ScotdataClean['Type'] = ScotdataClean['title'].str.extract(r'.\\.(P|C|G|A|F|M|I)\\.')[0]# checking for .(one of my letters). and exporting only that part\n",
    "\n",
    "ScotdataClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tlWKsFlBazKP",
    "outputId": "c895fe10-9370-4210-a015-6fc1133e63cf"
   },
   "outputs": [],
   "source": [
    "ScotdataClean.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JDR-wcOa3qS"
   },
   "source": [
    "This is useful but what if we also want a column that will explicitate what the type stands for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fT5Fhf3bbDPI"
   },
   "outputs": [],
   "source": [
    "# Define a mapping dictionary for the types to descriptive text\n",
    "type_map = {\n",
    "    'P': 'Parish',\n",
    "    'C': 'Miscellanea',\n",
    "    'G': 'General Observations',\n",
    "    'A': 'Appendix',\n",
    "    'F': 'General',\n",
    "    'M': 'Map',\n",
    "    'I': 'Index'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lh8qvc31bH2V",
    "outputId": "f76fbf55-4317-45fc-ff49-6e95502d5c46"
   },
   "outputs": [],
   "source": [
    "#Apply the mapping\n",
    "ScotdataClean['TypeDescriptive'] = ScotdataClean['Type'].map(type_map)\n",
    "\n",
    "ScotdataClean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV-I-9RwbqbE"
   },
   "source": [
    "#### 2.2 RecordId\n",
    "I want the first bit of the title as the RecordId of the document\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VvFskxjUb6aP",
    "outputId": "535246dc-ccca-4bd3-8324-e3ebb4f73856"
   },
   "outputs": [],
   "source": [
    "# when Regex expression starts becoming quite long you can define the pattern and then call it\n",
    "pattern = r'(StAS(\\.[a-z]?\\d+[a-z]?)*)\\.(P|C|G|A|F|M|I)\\.([^\\.]+)\\.'\n",
    "\n",
    "ScotdataClean['RecordID'] = ScotdataClean['title'].str.extract(pattern)[0]\n",
    "ScotdataClean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eu8Y1LSchQN"
   },
   "source": [
    "Now that is a scary Regex function but let's break it down\n",
    "\n",
    "1. **StAS**: This is a literal string. The regex matches sequences that begin with the characters 'StAS'. This likely represents a fixed identifier or prefix in your data.\n",
    "2. **(\\\\.[a-z]?\\d+[a-z]?)**\n",
    "\\\\.: This matches a literal dot ('.'). In regex, a dot is a special character that typically matches any character, so it needs to be escaped with a backslash (\\) to denote a literal dot.\n",
    "[a-z]?: This matches zero or one lowercase letter. The ? quantifier makes the preceding token in the regex (in this case, [a-z], which is any lowercase letter) optional.\n",
    "\\d+: This matches one or more digits. \\d is a shortcut for [0-9], and the + quantifier means \"one or more\" of the preceding element.\n",
    "[a-z]?: Again, this matches zero or one lowercase letter, similar to earlier in the pattern.\n",
    "This part of the expression (\\\\.[a-z]?\\d+[a-z]?) deals with sections in your string that are expected to follow a dot, potentially a letter, a sequence of digits, optionally followed by another letter. This can occur multiple times or not at all as indicated by the enclosing group and * quantifier.\n",
    "\n",
    "3. *: This quantifier matches zero or more of the preceding element (in this case, the entire group \\\\.[a-z]?\\d+[a-z]? ), allowing for flexibility in how many such sequences appear.\n",
    "4. **\\\\.(P|C|G|A|F|M|I)**\n",
    "\\\\.: As before, this matches a literal dot.\n",
    "(P|C|G|A|F|M|I): This is a capturing group that matches one of the specified uppercase letters. Each letter is separated by a pipe (|), which acts as an \"or\" operator in regex. This part is designed to match categorizations or types that follow your numeric sequences.\n",
    "5. **\\\\.**\n",
    ": This, again, matches a literal dot following the category character.\n",
    "6. **([^\\\\.]+)**\n",
    "([^\\\\.]+): This captures one or more characters that are not a dot. The caret (^) inside the square brackets negates the set, so [^\\\\.] means any character but a dot. The + quantifier ensures one or more characters are captured.\n",
    "This part is typically used to grab the next segment of the title which could act as an identifier or significant descriptor, not split by dots.\n",
    "7. **Final \\\\. (dot)**\n",
    "Ends with expecting a literal dot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQtI61cBcqRw"
   },
   "source": [
    "#### 2.3 Area\n",
    "\n",
    "I also want to extract the area that is the bit after p/c/g/a/f/m/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QDgMRpLtc0I2",
    "outputId": "93bcf91d-9937-4859-8e96-ca9f80bb46c1"
   },
   "outputs": [],
   "source": [
    "pattern = r'(StAS(\\.[a-z]?\\d+[a-z]?)*)\\.(P|C|G|A|F|M|I)\\.([^\\.]+)\\.'\n",
    "\n",
    "ScotdataClean['Area'] = ScotdataClean['title'].str.extract(pattern)[3]\n",
    "ScotdataClean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K__Ju2rsdg6V"
   },
   "source": [
    "### Parish\n",
    "The last bit that we need to extract is the parish that is the last bit up until the last fullstop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "z6rOiVp_dj6G",
    "outputId": "82e44f5d-7de3-4aae-f5db-d3858d13cc2a"
   },
   "outputs": [],
   "source": [
    "ScotdataClean['Parish'] = ScotdataClean['title'].str.split('.').str[-1]\n",
    "ScotdataClean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8r9IogMeVnq"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "- Can you use the code above to extract the area or the parish?\n",
    "- How would you do so?\n",
    "- Do you think you can use this for the id as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blCb5xLuef4S"
   },
   "outputs": [],
   "source": [
    "#ScotdataClean['Area'] = ScotdataClean['title'].str.split('.').str[\"enter value here\"]\n",
    "#ScotdataClean['Parish'] = ScotdataClean['title'].str.split('.').str[\"enter value here\"]\n",
    "#ScotdataClean['RecordID'] = ScotdataClean['title'].str.split('.').str[\"enter value here\"]\n",
    "\n",
    "#ScotdataClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "driBzotFgHWY"
   },
   "outputs": [],
   "source": [
    "# ScotdataClean['RecordID'] = ScotdataClean['RecordID'].apply(lambda x: '.'.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqaRrteegarh"
   },
   "source": [
    "Now our dataset is much clenaer and all our metadata are correctly encoded in separate columns in our dataset.\n",
    "For this workshop we are really just interested in the texts that describe the life in the parishes across scotland rather than indexes and other type of texts so we are going to work from now on a new dataset named 'Parish' that contains only the Type P text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZItvvXvghVv"
   },
   "outputs": [],
   "source": [
    "Parish = ScotdataClean[ScotdataClean['Type'] == 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "wlpqUCm5goeI",
    "outputId": "7fe7d531-99f5-4013-9a42-9c9034412e32"
   },
   "outputs": [],
   "source": [
    "# Getting descriptive statistics\n",
    "Parish.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNLEzHHE_MrX"
   },
   "outputs": [],
   "source": [
    "Parish.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkNyiI55hnLN",
    "outputId": "3e731ada-ad04-450e-9a3b-8e55ae7c03ef"
   },
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "print((Parish == '').sum())\n",
    "\n",
    "# Remove rows where 'text' is empty or contains only white spaces\n",
    "Parish = Parish[~Parish['text'].str.strip().eq('')]\n",
    "\n",
    "#Check for null values again\n",
    "print((Parish == '').sum())\n",
    "\n",
    "#ok we are good to go now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNszV9CPhsVi"
   },
   "outputs": [],
   "source": [
    "#export the new dataset\n",
    "Parish.to_csv('Parish.csv', index=False)\n",
    "#NB we are going to overwrite the Parish.csv that is currently in our directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElsM237Lh7qQ"
   },
   "source": [
    "## Explore the dataset we have created\n",
    "\n",
    "Now that we have a clener dataset from the orignal one we can start explore a bit better what we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cu7_8DWKiWg8"
   },
   "outputs": [],
   "source": [
    "# if you get an error when running the next cell it is probably because you have forgotten to import nltk. Without getting back to the top of this notebook you can uncomment the two lines below\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "#from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WpVE2A-iSjN"
   },
   "outputs": [],
   "source": [
    "# Create a function to count tokens\n",
    "def tokenize(text):\n",
    "    return len(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt51O0Enie_P"
   },
   "outputs": [],
   "source": [
    "# Tokenize the texts and create a DataFrame with tokens\n",
    "Parish['Tokens'] = Parish['text'].apply(tokenize)\n",
    "#Create a new object named TokenScotland\n",
    "TokenScotland = pd.DataFrame({\n",
    "    \"Tokens\": Parish['Tokens'],\n",
    "    \"Title\": Parish['title'],\n",
    "    \"Area\": Parish['Area'],\n",
    "    \"Parish\": Parish['Parish']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "-p1636mE0TeF",
    "outputId": "4414d701-8cd5-47d0-cdf2-a7130ae182ab"
   },
   "outputs": [],
   "source": [
    "#Let's see what we have created\n",
    "TokenScotland.info()\n",
    "TokenScotland.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "3I9lqikB-XVl",
    "outputId": "69c94904-db6b-4608-95da-3d7454e96b9d"
   },
   "outputs": [],
   "source": [
    "TokenScotland.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZSMGoBn0wG4"
   },
   "outputs": [],
   "source": [
    "# Now we group by areas\n",
    "BreakoutScotland = TokenScotland.groupby('Area').agg(\n",
    "    NReports=('Tokens', 'size'),  # Count of reports per area\n",
    "    MeanTokens=('Tokens', 'mean') # Mean tokens per area\n",
    ").reset_index()\n",
    "BreakoutScotland['MeanTokens'] = BreakoutScotland['MeanTokens'].round()  # Round the mean tokens\n",
    "BreakoutScotland['MeanTokens'] = BreakoutScotland['MeanTokens'].astype(int)# save it as an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l3PkqAw263Lm",
    "outputId": "84b73f48-fa98-47c5-ec10-b55abfdfab60"
   },
   "outputs": [],
   "source": [
    "# Let's look at what we have created\n",
    "BreakoutScotland.head()\n",
    "\n",
    "BreakoutScotland.sort_values(by='NReports', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K09eFY67eyY"
   },
   "source": [
    "This is ok to start thinking about the differences but we can also use data visualisation to get a better idea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "6qqNTAWP7lMd",
    "outputId": "800e9765-1a8e-4e13-e068-860629c1cea3"
   },
   "outputs": [],
   "source": [
    "# Ensure that seaborn's aesthetics are used\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(16, 8)) #size of the plot\n",
    "scatter = sns.scatterplot(data=BreakoutScotland,\n",
    "                          x='Area',\n",
    "                          y='NReports',\n",
    "                          size='MeanTokens',\n",
    "                          hue='MeanTokens',\n",
    "                          palette='viridis',\n",
    "                          sizes=(200, 2000),\n",
    "                          alpha=0.6,\n",
    "                          edgecolor='black')\n",
    "\n",
    "# Adding text labels for MeanTokens over the points\n",
    "for line in range(0, BreakoutScotland.shape[0]): # BreakoutScotland.shape[0] gives the total number of rows in the DataFrame.\n",
    "    scatter.text(BreakoutScotland.Area[line],# This gets the value from the 'Area' column of the current row (denoted by 'line')and sets it as the x-coordinate where the text will be placed on the scatter plot.\n",
    "                 BreakoutScotland.NReports[line], # This gets the value from the 'NReports' column of the current row and sets it as the y-coordinate where the text will be placed on the scatter plot.\n",
    "                 BreakoutScotland.MeanTokens[line],#This gets the value from the 'MeanTokens' column of the current row. This value is the text that will be displayed at the specified x,y coordinate on the plot.\n",
    "                 horizontalalignment='center', # 'center' aligns the text such that the center of the text string is at the x-coordinate.\n",
    "                 size='small',\n",
    "                 color='black',\n",
    "                 weight='semibold')# Sets the weight (or thickness) of the font of the text. 'semibold' is typically thicker than normal but not as thick as bold.\n",
    "\n",
    "# Enhance aesthetics\n",
    "plt.title('Number of Reports and Tokens in the Scotland Archive')\n",
    "plt.xlabel('Areas')\n",
    "plt.ylabel('Number of Reports')\n",
    "plt.xticks(rotation=90)  # Rotate the x-axis labels for better readability\n",
    "plt.legend(title='Mean of Tokens', loc='upper left', bbox_to_anchor=(1,1))  # Move legend outside the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rffxdDtX_oAL"
   },
   "source": [
    "## Exercise 3\n",
    "- What can you infer from the graph?\n",
    "- Can you identify possible issues we are going to have when doing the analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFvd7vPfLI1a"
   },
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwQyIBfnQxtI"
   },
   "outputs": [],
   "source": [
    "# if the beleow throw you an error because you do not have the Parish object anymore we can re-import it\n",
    "\n",
    "# URL of the raw CSV file\n",
    "#url = \"https://raw.githubusercontent.com/DCS-training/StatAccountScotland/refs/heads/main/Parish.csv\"\n",
    "\n",
    "# Use pandas to read the CSV file directly from the URL\n",
    "#Parish = pd.read_csv(url)\n",
    "#Parish.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Gw_0FHXZQ0HI",
    "outputId": "e00fc748-e118-4157-c3d5-c42b45c596fd"
   },
   "outputs": [],
   "source": [
    "Parish['Wordtokens'] = Parish['text'].apply(lambda x: word_tokenize(x.lower())) #A lambda function is an anonymous function, defined on-the-fly, and is useful for succinctly expressing small functions.\n",
    "Parish.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OMn9V3p_R4QK",
    "outputId": "939cb6b7-bbb1-491a-fbcd-5a68542e8201"
   },
   "outputs": [],
   "source": [
    "# Tokenise by Sentences\n",
    "Parish['Sentencetokens'] = Parish['text'].apply(lambda x: sent_tokenize(x.lower()))\n",
    "Parish.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyoQx5m8SRAX"
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "- Compare Wordtokens and Sentencetokens, how was the text divided?\n",
    "- Can you think to when use one or the other?\n",
    "\n",
    "You can use the code below to print out specific rows within Sentencetokens or Wordtokens and to check the length of each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvrnURgPS2hY",
    "outputId": "b8de9cd1-c55c-4916-d55b-0477a20d2b0a"
   },
   "outputs": [],
   "source": [
    "print(Parish['Sentencetokens'].iloc[1])\n",
    "len(Parish['Sentencetokens'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBrx9vmmTI1b"
   },
   "outputs": [],
   "source": [
    "print(Parish['Wordtokens'].iloc[1])\n",
    "len(Parish['Wordtokens'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ib4J8uVLTMqF"
   },
   "source": [
    "For now let's work on the wordtokens. The tokeniser did a good job in finding our words but we can clean it a bit better still.\n",
    "\n",
    "As usual the process is to create a function that does what we want and then apply it to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPl0bAS9TPs6"
   },
   "source": [
    "### 1. Clean the tokens\n",
    "The first step would be to create a function that we want to use and then map it on our dataset\n",
    "The step we want to do are\n",
    "- Removing the web addresses\n",
    "- Remove punctuation and symbols\n",
    "- Split the hypheneted words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wE6kGVLDTZjD"
   },
   "outputs": [],
   "source": [
    "# Function to clean the tokens\n",
    "def clean_tokens(tokens):\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        # Remove any URLs found in the token\n",
    "        token = re.sub(r'https?://\\S+|www\\.\\S+', '', token) # this step need to be done first or the next one amy remove bits of web addresses making it harder to remove them\n",
    "\n",
    "        # Remove punctuation and symbols (except hyphens for special handling), and all digits\n",
    "        token = re.sub(r'[^\\w-]', '', token)\n",
    "\n",
    "        # Split on hyphen and handle each part separately\n",
    "        parts = token.split('-')\n",
    "        for part in parts:\n",
    "            # Now applying strip here to each individual part after all other processing\n",
    "            part = part.strip()\n",
    "            # Remove numbers from each part only after splitting and stripping\n",
    "            part = re.sub(r'\\d+', '', part)  # Remove all digits\n",
    "            # Only add the part to the cleaned_tokens if it is not empty\n",
    "            if part:\n",
    "                cleaned_tokens.append(part)\n",
    "\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0t6mbR1UDoE"
   },
   "outputs": [],
   "source": [
    "Parish['Wordtokens'] = Parish['Wordtokens'].apply(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ag6e0FCUkJk",
    "outputId": "2700e070-a2e6-4ef5-b104-6c3be827ecf1"
   },
   "outputs": [],
   "source": [
    "#Let's check our results\n",
    "print(Parish['Wordtokens'].iloc[1])\n",
    "len(Parish['Wordtokens'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuNnKg3WU_U7"
   },
   "source": [
    "## Stopword\n",
    "Stop words are a set of commonly used words in a language. Examples of stop words in English are “a,” “the,” “is,” “are,” etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information.\n",
    "\n",
    "There are two approaches to this: use the embedded remove stopword function or work on the lenght of our tokens\n",
    "\n",
    "E.g. we can remove short words Remove tokens under 3 characters. (Shorter words won’t tell us much about our data, and because we removed punctuation, we want to get rid of the truncated contractions–e.g. I’m –>‘I’, ‘m’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxKG2-ApVDKS"
   },
   "outputs": [],
   "source": [
    "# Define a function to filter out tokens with fewer than 3 characters\n",
    "def filter_tokens(tokens):\n",
    "    return [token for token in tokens if len(token) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a2YV3bBVFIk"
   },
   "outputs": [],
   "source": [
    "# Apply this filter to the DataFrame's 'tokens' column\n",
    "Parish['filtered_tokens'] = Parish['Wordtokens'].apply(filter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVUZPiYrVK8e",
    "outputId": "30b6a060-19ca-457a-dd4b-2d232b4105be"
   },
   "outputs": [],
   "source": [
    "print(Parish['filtered_tokens'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZxZoNNlVSOx"
   },
   "source": [
    "now let's remove the English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwLSnvf9VXlR"
   },
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irCpQJpSVbk9"
   },
   "outputs": [],
   "source": [
    "# embedding both <3 character and stop words+\n",
    "def filter_tokens(tokens):\n",
    "    return [token for token in tokens if len(token) >= 3 and token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKQUwEFIVf4Q"
   },
   "outputs": [],
   "source": [
    "# Apply this filter to the DataFrame's 'tokens' column\n",
    "Parish['Wordtokens'] = Parish['Wordtokens'].apply(filter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wUU0RTVVpMD",
    "outputId": "c9d964f9-33ca-48c7-fbd7-931db91a876a"
   },
   "outputs": [],
   "source": [
    "print(Parish['Wordtokens'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNenxkpSVtPw"
   },
   "source": [
    "## Exercise 5\n",
    "Can you think to possible downsides of removing the stop words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "776r3fPHVzd6"
   },
   "source": [
    "The last thing we are going to have a look at is how to remove a custom list of words that we do not want to have in our tokens because too popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2G4ASsSVzI3"
   },
   "outputs": [],
   "source": [
    "custom_stopwords = ['statistical', 'account', 'parish','one', 'year', 'years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qv9dl1zV4ke"
   },
   "outputs": [],
   "source": [
    "def filter_custom_tokens(tokens, custom_stopwords):\n",
    "    custom_stopwords_set = set(custom_stopwords)  # Convert list to set for faster lookup\n",
    "    return [token for token in tokens if token not in custom_stopwords_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-C3wvOLEV7a9"
   },
   "outputs": [],
   "source": [
    "# Apply this filter to the DataFrame's 'tokens' column\n",
    "Parish['Wordtokens'] = Parish['Wordtokens'].apply(lambda tokens: filter_custom_tokens(tokens, custom_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4PoyjXdV9Ot",
    "outputId": "c1e70989-1539-4c8b-e622-5c4950934a57"
   },
   "outputs": [],
   "source": [
    "print(Parish['Wordtokens'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smF_woNJWPu8"
   },
   "source": [
    "## Lemming and Stemming\n",
    "Ok we are going to look now a last 'cleaning' step we can perform: the lemmatisation and stemmatisation. AS for the stopwords these need to be use with care cause they can alter the meaning of words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvcUlfSZWWvy"
   },
   "outputs": [],
   "source": [
    "# First the stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Creating the stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgUK7MFnWtip"
   },
   "outputs": [],
   "source": [
    "Parish['StemmingToken'] = Parish['Wordtokens'].apply(lambda x: [stemmer.stem(token) for token in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "8B4WuG3aW0cg",
    "outputId": "c8c97f95-9e1c-45dc-a546-289d3d75f857"
   },
   "outputs": [],
   "source": [
    "Parish.iloc[0:10,[8,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cJo0y_wW37j",
    "outputId": "f0179569-9927-4007-cb54-ed5a072dbd04"
   },
   "outputs": [],
   "source": [
    "# Now with Lemming\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Downloading necessary resources from NLTK\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# Creating the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9ZPqNO7Xwyt"
   },
   "outputs": [],
   "source": [
    "Parish['LemmTokens'] = Parish['Wordtokens'].apply(lambda x: [lemmatizer.lemmatize(token, pos='v') for token in x])#For better lemmatization, especially for verbs, you can specify the part-of-speech (POS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "bpCpHs39X3zq",
    "outputId": "948c2360-cb2e-45b6-88e3-5748f88432e9"
   },
   "outputs": [],
   "source": [
    "Parish.iloc[10:20,[8,11,12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ltu3aZkSYJ_O"
   },
   "source": [
    "## Exercise 6\n",
    "- Can you tell the difference between the different strategies?\n",
    "- What do you think are the pro and con?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_MbqP5CYVGf"
   },
   "source": [
    "This is the end of the first part! To make sure that we are going to restart on the same dataset let's export it as a .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZgr2OhtYfYW"
   },
   "outputs": [],
   "source": [
    "# We can also save our new dataframe as a.csv file so that we will be easily re-import it\n",
    "Parish.to_csv('Parish.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
