{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqOnUJHwv6nz"
   },
   "source": [
    "# Analysis\n",
    "\n",
    "In this notebook we are going to explore different techniques to analyse our dataset and how we can\n",
    "The first thing that we need to do is to import all the libraries that we are going to use.\n",
    "\n",
    "\n",
    "## Set Up\n",
    "\n",
    "For this second Notebook we need:\n",
    "\n",
    "- **pandas**: The pandas module provides support to work with datasets e.g. data cleaning andand transformation.\n",
    "- **nltk**: The NLTK library allows you to work with human language data (natural language processing). It includes libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning\n",
    "- **numpy**: The numpy module is for scientific computing with Python. This will help with performing linear algebra and mathematic operations on your dataset.\n",
    "- **matplotlib**: The matplotlib module will allow you to create visualisations.\n",
    "- **seaborn**: The matplotlib module will allow you to create visualisations\n",
    "- **wordcloud**: The wordcloud module will allow you to create wordcloud visualisations.\n",
    "- **ast**: Ast is a powerful tool for interacting with and modifying Python code at a structural level.\n",
    "- **collections** The collections library in Python offers specialized container datatypes that enhance the functionality, efficiency, and ease of use beyond Python's built-in containers like lists, dictionaries, and tuples.The defaultdict is a subclass of the standard Python dictionary and allows specifying a default value for missing keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWPdvHuQIQMJ",
    "outputId": "596229e0-8101-436f-9fbc-e15f4f8fed14"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import defaultdict\n",
    "import pandas as pd # We are renaming pandas as pd to be faster in calling it\n",
    "import nltk\n",
    "nltk.download('punkt_tab')# within nltk we need to download or import a series of submodules. Punkt is used for tokenizing sentences\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize # This is a specific import from the nltk.tokenize module, bringing in the word_tokenize function, which is used to split text into words.\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.text import Text # need for keyword in context\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ep9RUgqwkXq"
   },
   "source": [
    "In theory you should still have the Parish object in your environment but if it passed some time between working on the first notebook and this notebook the kernel could have restarted.\n",
    "\n",
    "Without having to do the cleaning and pre-processing again we can import the file directly from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "Q3itnh5awNJl",
    "outputId": "19fa0b2b-9d32-4ed3-b4c8-db5d9b8be841"
   },
   "outputs": [],
   "source": [
    "# URL of the raw CSV file\n",
    "url = \"https://github.com/DCS-training/StatAccountScotland/blob/main/ParishTokenised.csv?raw=true\"\n",
    "\n",
    "# Use pandas to read the CSV file directly from the URL\n",
    "Parish = pd.read_csv(url)\n",
    "# Convert the 'Wordtokens' column from string to actual list\n",
    "Parish['Wordtokens'] = Parish['Wordtokens'].apply(ast.literal_eval) #here we are using that ast module we mentioned above to make sure variable are recording properly\n",
    "\n",
    "Parish.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjKQi2co0t3o"
   },
   "source": [
    "Good! Now that we have done all the hard cleaning and pre-processing bit, let's start having a look at the analysis we can perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_1CNU2n08wl"
   },
   "source": [
    "## 1 Keyword Frequency\n",
    "The first type of analysis we are going to look at is the frequency of keyword.\n",
    "\n",
    "Within this topic we are going to explore the frequency distribution of keywords\n",
    "and method to visualise it (e.g. Wordclouds). Then we are going to look at the Term Frequency-Inverse Document Frequency or TF-IDF. TF-IDF is a numerical statistic used to indicate how important a word is to a document in a collection or corpus, adjusting the term frequency by how common the word is across all documents.\n",
    "\n",
    "### 1a Frequency Distribution:\n",
    "This is the most direct method in NLTK to handle keyword frequency. You can use the FreqDist class from NLTK to create a frequency distribution, which is essentially a count of each vocabulary item in the text.\n",
    "\n",
    "The first thing we need to do is to flat our token to a single list of tokens across the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHek2sEl046N",
    "outputId": "145b5e0c-a4d1-4139-a26d-1a772e11627e"
   },
   "outputs": [],
   "source": [
    "Tokens= Parish['Wordtokens'].tolist()\n",
    "Tokens[0:15]\n",
    "# Flatten the list of lists 'Tokens'\n",
    "flat_tokens = [token for sublist in Tokens for token in sublist]\n",
    "flat_tokens[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObY6ZIGK2Mlb",
    "outputId": "7636d0d3-438d-47fe-efae-c5d24341defb"
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(flat_tokens)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U80KwSjc36fA"
   },
   "source": [
    "**Samples**: Refers to the unique tokens (or items) that were counted by FreqDist. In your case, \"132623 samples\" means there are 132,623 unique words or tokens in the dataset you analyzed.\n",
    "\n",
    "**Outcomes**: Refers to the total number of tokens (or events, items, etc.) that were processed â€” essentially, the total count of all occurrences of all tokens in the data. \"4697485 outcomes\" indicates that, summing the counts of all individual tokens, there were 4,697,485 tokens processed in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "4w6Vninp35XO",
    "outputId": "f14cc10c-32a5-45b3-9442-c4790c18ef71"
   },
   "outputs": [],
   "source": [
    "# Plot the top 20 most common tokens\n",
    "fdist.plot(20, title='Top 20 Most Common Tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzrU64DNwFnV"
   },
   "source": [
    "### 1b Wordcloud\n",
    "A word cloud (also known as a tag cloud or word art) is a visual representation of text data where the size of each word indicates its frequency or importance within a large body of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-Bf8arew3qb"
   },
   "outputs": [],
   "source": [
    "# Convert FreqDist to a dictionary for word cloud generation\n",
    "freq_dict = dict(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "iGq6XmQZxFmI",
    "outputId": "2cd59e44-83ee-4938-fb63-6edd384a0757"
   },
   "outputs": [],
   "source": [
    "# Create the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq_dict)\n",
    "\n",
    "# Display the word cloud\n",
    "fig, ax = plt.subplots(figsize=(14, 7))  # Define the figure\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis('off')  # Turn off axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1VgxrBLxRp4"
   },
   "outputs": [],
   "source": [
    "# Save the figure to a file\n",
    "fig.savefig('wordcloud.jpg', format='jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UainwPugxVY0"
   },
   "source": [
    "Ok this is on the full dataset but it would probably more interesting to compare two areas on the dataset e.g one very city-like and one very remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oVxE80g9xZP-",
    "outputId": "396b464f-8857-4631-8e76-756c0b8aa504"
   },
   "outputs": [],
   "source": [
    "print(Parish['Area'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMDZIihIxdJY"
   },
   "outputs": [],
   "source": [
    "# Subsetting data for Edinburgh\n",
    "edinburgh_tokens = Parish[Parish['Area'] == 'Edinburgh']['Wordtokens'].explode().tolist()\n",
    "\n",
    "# Subsetting data for Fife\n",
    "fife_tokens = Parish[Parish['Area'] == 'Fife']['Wordtokens'].explode().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llENAa3UxfWS"
   },
   "outputs": [],
   "source": [
    "# Generate frequency distributions\n",
    "freq_dist_edinburgh = FreqDist(edinburgh_tokens)\n",
    "freq_dist_fife = FreqDist(fife_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b_mBjXXIxllP",
    "outputId": "fdadb564-faa0-4bd5-ca3d-ff5219a1b456"
   },
   "outputs": [],
   "source": [
    "# to avoid having to repeat the steps multiple time we can create a function that will contain the specificsof creating and saving the visualisations\n",
    "\n",
    "def create_plots(freq_dist, area_name):\n",
    "    # Creating the frequency plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    freq_dist.plot(30, title=f'Top 30 Most Frequent Words in {area_name}')\n",
    "    plt.savefig(f'freq_plot_{area_name.lower()}.jpg')  # Save the frequency plot as a JPEG\n",
    "\n",
    "    # Creating the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq_dist)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for {area_name}')\n",
    "    plt.savefig(f'wordcloud_{area_name.lower()}.jpg')  # Save the word cloud as a JPEG\n",
    "\n",
    "# Create and save plots for Edinburgh\n",
    "create_plots(freq_dist_edinburgh, 'Edinburgh')\n",
    "\n",
    "# Create and save plots for Fife\n",
    "create_plots(freq_dist_fife, 'Fife')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtQ_Ka8lxval"
   },
   "source": [
    "Can you see any big difference? Why do you think you are getting quite different results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-M2KQvixx0e"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Try repeat the steps above for a different couple of areas.\n",
    "\n",
    "**Tip:** we can reuse the function we created to create the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noT4CPnxx5UH"
   },
   "outputs": [],
   "source": [
    "# enter your code below\n",
    "\n",
    "# Subsetting data for an area\n",
    "#_tokens = Parish[Parish['Area'] == '_']['Wordtokens'].explode().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy25mbEZAAZa"
   },
   "source": [
    "### 1c Term Frequency-Inverse Document Frequency\n",
    "![TF-IDF.png](https://miro.medium.com/v2/resize:fit:720/format:webp/0*a-1asDQprowFNGIe.png)\n",
    "\n",
    "From: https://nachi-keta.medium.com/nlp-what-is-tf-idf-149bc6a1da78\n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency.\n",
    "It reflects the importance of a word in a document within a collection or corpus.\n",
    "\n",
    "TF-IDF takes into account both the frequency of a word within a document (term frequency) and the rarity of the word across the entire corpus (inverse document frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7sOxy3fCGUQ",
    "outputId": "6498226c-c6d4-453f-c7c4-c7d27b2e9d57"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# We are doing this on a subset cause our dataset is too big\n",
    "caithness_texts = Parish[Parish['Area'] == 'Caithness']['text'][:3]\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(caithness_texts)\n",
    "\n",
    "# Create a DataFrame to view the TF-IDF data clearly\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df_tfidf)\n",
    "\n",
    "# To see the IDF for each word\n",
    "print(\"\\nIDF for each word:\")\n",
    "for word, idf in zip(vectorizer.get_feature_names_out(), vectorizer.idf_):\n",
    "    print(f\"{word}: {idf}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvHen8Scf6p7"
   },
   "source": [
    "This is very complicated to decode giving how large our dataset still is.\n",
    "\n",
    "Let's have a try on an artificial smaller dataset so it will be easier to understand the principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QqMuLxtB7ys",
    "outputId": "5875d15f-39b7-4744-ae72-a94102e4c1ec"
   },
   "outputs": [],
   "source": [
    "# Generating sample documents\n",
    "np.random.seed(0)  # For reproducible results\n",
    "\n",
    "# Basic words list (excluding the stop words)\n",
    "words = [\"apple\", \"banana\", \"carrot\", \"date\", \"eggplant\", \"fig\", \"grape\"]\n",
    "# Stop words that will appear in every document\n",
    "stop_words = [\"the\", \"and\", \"is\"]\n",
    "\n",
    "# Generate documents with each having a mix of randomly selected words and all stop words\n",
    "docs = [\" \".join(list(np.random.choice(words, size=np.random.randint(2, 4), replace=False)) + stop_words)\n",
    "        for _ in range(30)]\n",
    "# Here's what some of the documents look like\n",
    "print(docs[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdvDVxIxrLuu"
   },
   "outputs": [],
   "source": [
    "# Compute the TF-IDF scores.\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Create a DataFrame to view the TF-IDF data clearly\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "o89xcTTQrXcd",
    "outputId": "e96d805b-4af0-4409-acfa-bc053f9399e6"
   },
   "outputs": [],
   "source": [
    "# Create a heatmap for the TF-IDF DataFrame\n",
    "plt.figure(figsize=(12, 10))  # Set the figure size for better visibility\n",
    "# Create the heatmap using seaborn\n",
    "sns.heatmap(df_tfidf,# 'df_tfidf' is a DataFrame where columns correspond to terms and rows to documents\n",
    "            annot=False,# 'annot=False' means we do not annotate each cell with numerical values to avoid clutter\n",
    "            cmap=\"viridis\",# 'cmap=\"viridis\"' sets the color map to 'viridis', which is a perceptually uniform colormap\n",
    "            cbar=True, # 'cbar=True' enables the color bar on the side, showing the scale of the TF-IDF values\n",
    "            xticklabels=vectorizer.get_feature_names_out(), # 'xticklabels=vectorizer.get_feature_names_out()' uses the terms as labels for the x-axis\n",
    "            linewidths=0.5, linecolor='black')# 'linewidths' and 'linecolor' are used to add borders to the cells\n",
    "\n",
    "# Set the title of the heatmap\n",
    "plt.title('TF-IDF Heatmap for 30 Documents')\n",
    "\n",
    "# Set the label for the x-axis\n",
    "plt.xlabel('Terms')\n",
    "\n",
    "# Set the label for the y-axis\n",
    "plt.ylabel('Document Index')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IWO6M1Mt-7G"
   },
   "source": [
    "## Exercise 2\n",
    "Using the code below explore the high and low values for each term what is telling us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "NqArU2Pbscq-",
    "outputId": "4b334885-9534-4340-e062-7780a0a7108a"
   },
   "outputs": [],
   "source": [
    "docs[14]\n",
    "#docs[23]\n",
    "#docs[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMV1dB7Lx_9u"
   },
   "source": [
    "## 2 Keywords in Context\n",
    "Keywords in context (KWIC) is a method of text analysis that involves displaying words from a specified text within the context of their surrounding words in a fixed number of characters or words. This approach, often used in concordance tools, helps in understanding how specific terms are used within a text, revealing patterns of usage and the semantic environments in which the keywords appear, aiding linguists and researchers in qualitative textual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEIvHVRtyZS-"
   },
   "outputs": [],
   "source": [
    "# Transforming the total of the token into a Text object\n",
    "text_content = Text(flat_tokens)\n",
    "# a \"Text\" object is a wrapper around a list of tokens (words), providing convenient methods to explore the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAoZNv5UeLUf"
   },
   "source": [
    "**NB** You have probably noticed by now tha different analyses and libraries require to manipulate to dataset to different formats. The wrong format of data is one of the most common reason why you get errors when try to apply functions to your dataset. Each library/function will have a page online that would describe the shape your dataset needs to have. So if in doubt google it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UVXadg3gDKM"
   },
   "source": [
    "### 2.1 Look at keywords in Context\n",
    "The first and easier step is to select a keyword you are interesting in exploring and see what is present in the text around it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pef9dpbyeIu",
    "outputId": "4a39b994-7b60-4171-ae0b-81e3988af839"
   },
   "outputs": [],
   "source": [
    "# Let's work for example at where intemperance is mentioned\n",
    "text_content.concordance('intemperance', lines=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCbzeVNWywqr"
   },
   "source": [
    "Ok this is interesting in itself but we will have to look case by case.\n",
    "\n",
    "Can we actually look at more recurring words around our keyword? i.e what are the most likely words to be found around intemperance? Unsurprisingly this will require yet some more data wrangling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8fh_wsHytNO"
   },
   "outputs": [],
   "source": [
    "# Fetching concordance lines which you can analyze or extract text from\n",
    "concordance_lines = text_content.concordance_list(\"intemperance\")\n",
    "\n",
    "# Correctly extracting and concatenating text from each ConcordanceLine object\n",
    "# Skipping the keyword itself because it will skew our analysis\n",
    "contexts = [' '.join(line.left + line.right) for line in concordance_lines]# the ' ' is fundamental or your words will be all squished together\n",
    "\n",
    "# Tokenize each context\n",
    "tokenized_contexts = [word_tokenize(context) for context in contexts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aYvH6jazKyZ"
   },
   "source": [
    "Let's look at what we have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0bKvaXFzNTW",
    "outputId": "1d655d72-1004-4b80-8019-e1f92110e3ae"
   },
   "outputs": [],
   "source": [
    "print(tokenized_contexts[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiGNt7BqzQUj"
   },
   "source": [
    "This is still a nested list so we need to flat them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "je_CY0uNzVQv"
   },
   "outputs": [],
   "source": [
    "flat_tokens_intemperance = [token for sublist in tokenized_contexts for token in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBZUineKzbyK"
   },
   "source": [
    "Now we can finally plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "Uu2830YCzeT_",
    "outputId": "b2d3ef8c-3c5c-4e26-998c-08669acfa3b9"
   },
   "outputs": [],
   "source": [
    "freq_dist = FreqDist(flat_tokens_intemperance)\n",
    "# Plot the top 20 most common tokens\n",
    "freq_dist.plot(20, title='Top 20 Most Common Tokens near intemperance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPXkTzRWznqT"
   },
   "source": [
    "### 2.2 Similar tokens\n",
    "\n",
    "We can also check similar token in context. Using similar(token) returns a list of words that appear in the same context as token.\n",
    "\n",
    "In this case the the context is just the words directly on either side of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cyT3Jxbz2GC",
    "outputId": "443d2a36-a786-44d1-fbed-50faa28267b8"
   },
   "outputs": [],
   "source": [
    "# First let's look at a new keyword == Church\n",
    "text_content.concordance('church', lines=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbuEHcyoz-6y",
    "outputId": "3097bf8d-5076-4cc0-9510-8852f1284583"
   },
   "outputs": [],
   "source": [
    "text_content.similar(\"church\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDqTbh-q05uQ"
   },
   "source": [
    "Using similar(token) returns a list of words that appear in the same context as token. In this case the the context is just the words directly on either side of token. So the list above is the list of words that has similar pre and after tokens to church e.g. \"ridge manse situate\".\n",
    "\n",
    "We can even check what these concordance words are for each couple of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEOAnP7z04-E",
    "outputId": "a120fd17-45d0-461f-ab17-b35670010d72"
   },
   "outputs": [],
   "source": [
    "text_content.common_contexts(['church', 'manse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFSPeeAF2BRX"
   },
   "source": [
    "## Exercise 3\n",
    "What do you think this is telling us about the similarities between church and manse in our text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvXTrYIf2LZs"
   },
   "source": [
    "## 3 Keyword Dispersion\n",
    "\n",
    "We can also see how specific keywords re-occour across our dataset.\n",
    "This is much more informative when you are analysing books (e.g. how often a carachters is named across the book) but it can be useful in our case too to look at specific keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "ENqH9W2z2nJ2",
    "outputId": "af47a8c7-e1e6-477a-edaf-3baffb91f0ee"
   },
   "outputs": [],
   "source": [
    "text_content.dispersion_plot([\"church\", \"sheep\", \"intemperance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 509
    },
    "id": "C_P22sib2y2H",
    "outputId": "f1804711-ca2b-4a8d-a994-4f7e2c00980b"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "# Load the text of Moby Dick from the NLTK Gutenberg corpus\n",
    "moby_dick_text = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(moby_dick_text)\n",
    "\n",
    "# Create a Text object\n",
    "text_object = Text(tokens)\n",
    "\n",
    "# List of specific words to create a dispersion plot for\n",
    "target_words = ['Ahab', 'whale', 'ship', 'sea', 'Ishmael']\n",
    "\n",
    "# Generate the dispersion plot\n",
    "text_object.dispersion_plot(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvJE06qU3rLF"
   },
   "source": [
    "## 4 Bi-grams, N-Grams\n",
    "When we talk about the context of words (or tokens!) in text analysis, we're referring to the surrounding words of a given word. Concordances show a bit of context to the left of an input word (just before the word appears) and to the right of that word (just after that word appeared).\n",
    "\n",
    "We can use the similar method to see words that appear in similar contexts, meaning they're surrounded by similar tokens, as the token we input.\n",
    "\n",
    "Pairs of words that occur together, such as \"good\" and \"opinion,\" are referred to as **bigrams**, where \"bi\" indicates two. **N-grams** are groups of words that occur together, where n is a number of your choice.\n",
    "\n",
    "To get all the bigrams in a text, we can use the bigrams() method, into which we pass the variable referring to the text itself.\n",
    "\n",
    "Bigrams allow us to see which words commonly co-occur within a given dataset, which can be particularly useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4LmyZMM3qcR",
    "outputId": "dc8be7b3-be1c-438c-8c99-34da718b1cf0"
   },
   "outputs": [],
   "source": [
    "bigrams_list = list(nltk.bigrams(text_content))\n",
    "print(bigrams_list[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3X-VS7cD3wvU",
    "outputId": "92c18a30-9776-442f-d1c0-5c1afc26ade9"
   },
   "outputs": [],
   "source": [
    "trigrams_list = list(nltk.trigrams(text_content))\n",
    "print(trigrams_list[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpqiHJuP36N4"
   },
   "source": [
    "## 5 Working with Geographical Data\n",
    "This is the last topic we are going to have a look.\n",
    "\n",
    "Our dataset contains geographical information so we can also analyse our data based on their geographical location.\n",
    "To do so we need a geographical file, in our case a geopackage file, (basically a vectorial file representing each area of Scotland). This file has been compiled in a way to match the areas in our dataset with the areas of historical parishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSdstYQW4AG5"
   },
   "outputs": [],
   "source": [
    "#! pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7NyFTQNhzFm"
   },
   "source": [
    "### Set Up\n",
    "Fot this last bit we need some additional packages that we haven't look yet.\n",
    "- **geopandas** : is similar to Pandas but specifically for geographical dataset (that will have different file extensions).\n",
    "- **matlibplot**: we have already import matlibplot but to print out geographical data we need some additional features that we need call separately\n",
    "- **io, request and PIL**: you will need these additonal packages to be able to read and import an image hosted on a website. The first one will help with the encoding, the second with request data from the website, and the last with the image itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f531L2Na4CgV"
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox # Import necessary classes\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaLVvI6O4Gey",
    "outputId": "70e19296-955b-4aa5-d894-52289bc06ccf"
   },
   "outputs": [],
   "source": [
    "# import hte .gpkg file\n",
    "url2 = \"https://github.com/DCS-training/StatAccountScotland/blob/main/Spatial/Parishes.gpkg?raw=true\"\n",
    "# Adjust the path to where your GeoPackage file is located\n",
    "ParishesGeo = gpd.read_file(url2, layer='civilparish_pre1891')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8Ln0w1T5NaR"
   },
   "source": [
    "Let's say that we want to focus on the mentions of ilness in our dataset. First we need to search all mention of our ilness-related keyword and create a new column in our dataset that would print either yes or no if one of our keyword is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T84KnmqO5Srq"
   },
   "outputs": [],
   "source": [
    "# if the beleow throw you an error because you do not have the Parish object anymore we can re-import it\n",
    "# URL of the raw CSV file\n",
    "#url = \"https://raw.githubusercontent.com/DCS-training/StatAccountScotland/refs/heads/main/Parish.csv\"\n",
    "# Use pandas to read the CSV file directly from the URL\n",
    "#Parish = pd.read_csv(url)\n",
    "#Parish.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXE-NUFK5WK7"
   },
   "outputs": [],
   "source": [
    "# Check for keywords and add them to a new column\n",
    "Parish['Illness'] = np.where(Parish['text'].str.contains('ill|illness|sick|cholera', case=False, regex=True), \"yes\", \"no\")\n",
    "\n",
    "# Aggregate data by Area - Group by the Illness column and geographical area\n",
    "IllnessGroup = Parish.groupby('Area').agg(\n",
    "    Total=('Illness', 'size'),\n",
    "    Count=('Illness', lambda x: np.sum(x == 'yes'))\n",
    ").reset_index()\n",
    "IllnessGroup['Percentage'] = round(IllnessGroup['Count'] / IllnessGroup['Total'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "due3smpV5Zyg"
   },
   "outputs": [],
   "source": [
    "# Perform the merge - ensuring similar keys are used for merging\n",
    "MergedIlness = ParishesGeo.merge(IllnessGroup, left_on='JOIN_NAME_', right_on='Area', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELNFTqVu5dUV"
   },
   "outputs": [],
   "source": [
    "# Create a color map\n",
    "color_palette = LinearSegmentedColormap.from_list(\"my_palette\", [\"white\", \"red\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "G_HoY1jB5hh9",
    "outputId": "91669034-6fa0-461d-d955-b1c20b2ca4a6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "MergedIlness.plot(column='Percentage',# that to use for colouring areas\n",
    "                  cmap=color_palette, # using the palette I created\n",
    "                  linewidth=0.8, # thinkness of the line\n",
    "               ax=ax,\n",
    "                  edgecolor='0',# colour of the lines\n",
    "                  missing_kwds={'color': 'lightgrey'})# how to deal with missing values\n",
    "ax.title.set_text('Illness Report Across Scotland')# title of the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D6uR0ER5sUG"
   },
   "source": [
    "we can also add a legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "id": "FCMQPL-W5oTY",
    "outputId": "c4835a51-3263-47ab-b4fa-d1ac20071b38"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "divider = make_axes_locatable(ax)# to see the axes\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)# cax = coloir axis. Create the legend\n",
    "\n",
    "\n",
    "MergedIlness.plot(column='Percentage',\n",
    "                  cmap=color_palette,\n",
    "                  linewidth=0.8,\n",
    "               ax=ax,\n",
    "                  edgecolor='0',\n",
    "                  missing_kwds={\n",
    "                   'color': 'lightgrey',\n",
    "                   'label': 'No data'\n",
    "               },\n",
    "               legend=True, cax=cax)# cax = coloir axis\n",
    "\n",
    "ax.title.set_text('Illness Report Across Scotland')\n",
    "ax.set_axis_off()  # Optionally turn off the axis.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y67VgfoJ6Mzj"
   },
   "source": [
    "Ok, let's now search for witches but this time I also want a scale bar and a north arrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKcb6d7ykdg4"
   },
   "source": [
    "Now let's repeat our steps again but with a different subject i.e. witches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WurP6X5E6Q86"
   },
   "outputs": [],
   "source": [
    "# Check for keywords and add them to a new column\n",
    "Parish['Witchcraft'] = np.where(Parish['text'].str.contains('witch|spell|witches|enchantemt|magic|witchcraft', case=False, regex=True), \"yes\", \"no\")\n",
    "\n",
    "# Aggregate data by Area - Group by the 'Witchcraft column and geographical area\n",
    "WitchcraftGroup = Parish.groupby('Area').agg(\n",
    "    Total=('Witchcraft', 'size'),\n",
    "    Count=('Witchcraft', lambda x: np.sum(x == 'yes'))\n",
    ").reset_index()\n",
    "WitchcraftGroup['Percentage'] = round(WitchcraftGroup['Count'] / WitchcraftGroup['Total'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSEvzToj6U9V"
   },
   "outputs": [],
   "source": [
    "# Perform the merge - ensuring similar keys are used for merging\n",
    "MergedWitches = ParishesGeo.merge(WitchcraftGroup , left_on='JOIN_NAME_', right_on='Area', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unV7Zn0k6W9E"
   },
   "outputs": [],
   "source": [
    "# Create a color map\n",
    "color_palette2 = LinearSegmentedColormap.from_list(\"my_palette\", [\"white\", \"purple\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0vnJb-_6sWD"
   },
   "outputs": [],
   "source": [
    "# to add the scale bar and north arrow I need a couple more features from matlibplot\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "id": "6mMAxmPF6cFT",
    "outputId": "0b42c0df-7284-4712-d49c-846ba6d8b7ed"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "divider = make_axes_locatable(ax)# to see the colour legend\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "MergedWitches.plot(\n",
    "    column='Percentage',\n",
    "    cmap=color_palette2,\n",
    "    linewidth=0.8,\n",
    "    ax=ax,\n",
    "    edgecolor='0',\n",
    "    legend=True,\n",
    "    cax=cax,\n",
    "    missing_kwds={\n",
    "        'color': 'lightgrey',\n",
    "        'label': 'No data'\n",
    "    }\n",
    ")\n",
    "\n",
    "ax.set_title('Witches Report Across Scotland')\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Add scale bar\n",
    "scalebar = AnchoredSizeBar(ax.transData,\n",
    "                           100000, '100 km', 'lower left',# the unit of measure is meter\n",
    "                           pad=0.4,\n",
    "                           color='black',\n",
    "                           frameon=False,\n",
    "                           size_vertical=4,\n",
    "                           fontproperties=fm.FontProperties(size=12))\n",
    "ax.add_artist(scalebar)\n",
    "\n",
    "# Add North Arrow\n",
    "x, y, arrow_length = 0.95, 0.1, 0.1\n",
    "ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),\n",
    "            arrowprops=dict(facecolor='black', width=5, headwidth=15),\n",
    "            ha='center', va='center', fontsize=12, xycoords=ax.transAxes)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znjOSKVA65mO"
   },
   "source": [
    "### Using multiple geographical files\n",
    "This is all nice but can we bring together multiple spatial information (e.g. shape and points)? Yes we can build up on our visualisation.\n",
    "\n",
    "To do so we need to look at a different subject i.e. whisky for which we have at our dispostion also a geographical file containing punctual locations of modern distilleries.\n",
    "\n",
    "Basically what we are trying to do is to see if the modern distilleries are mostly located in places where more mentions of whisky and alchool were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boPr4P5f7GtS"
   },
   "outputs": [],
   "source": [
    "# Let's now search for wisky mentions\n",
    "# Check for keywords and add them to a new column\n",
    "Parish['Wisky'] = np.where(Parish['text'].str.contains('illicit still|illicit distillery|drunk|intemperance|wisky|whisky|whiskey|whysky|alembic', case=False, regex=True), \"yes\", \"no\")\n",
    "\n",
    "# Aggregate data by Area - Group by the 'Wisky column and geographical area\n",
    "WiskyGroup = Parish.groupby('Area').agg(\n",
    "    Total=('Wisky', 'size'),\n",
    "    Count=('Wisky', lambda x: np.sum(x == 'yes'))\n",
    ").reset_index()\n",
    "WiskyGroup['Percentage'] = round(WiskyGroup['Count'] / WiskyGroup['Total'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDlA-eeE7k52"
   },
   "outputs": [],
   "source": [
    "# Perform the merge - ensuring similar keys are used for merging\n",
    "MergedWisky = ParishesGeo.merge(WiskyGroup , left_on='JOIN_NAME_', right_on='Area', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NVAXjx97phD"
   },
   "outputs": [],
   "source": [
    "# Create a color map\n",
    "color_palette3 = LinearSegmentedColormap.from_list(\"my_palette\", [\"white\", \"brown\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 992
    },
    "id": "jvt-fyic7tHL",
    "outputId": "78317719-2d07-4743-ca05-d7e2f2dd2b2c"
   },
   "outputs": [],
   "source": [
    "# Plot it\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "MergedWisky.plot(\n",
    "    column='Percentage',\n",
    "    cmap=color_palette3,\n",
    "    linewidth=0.8,\n",
    "    ax=ax,\n",
    "    edgecolor='black',\n",
    "    legend=True,\n",
    "    cax=cax,\n",
    "    missing_kwds={\n",
    "        'color': 'lightgrey',\n",
    "        'label': 'No data'\n",
    "    }\n",
    ")\n",
    "\n",
    "ax.set_title('Whisky Report Across Scotland')\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Add scale bar\n",
    "scalebar = AnchoredSizeBar(ax.transData,\n",
    "                           100000, '100 km', 'lower left',\n",
    "                           pad=0.4,\n",
    "                           color='black',\n",
    "                           frameon=False,\n",
    "                           size_vertical=4,\n",
    "                           fontproperties=fm.FontProperties(size=12))\n",
    "ax.add_artist(scalebar)\n",
    "\n",
    "# Add North Arrow\n",
    "x, y, arrow_length = 0.95, 0.1, 0.1\n",
    "ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),\n",
    "            arrowprops=dict(facecolor='black', width=5, headwidth=15),\n",
    "            ha='center', va='center', fontsize=12, xycoords=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Hf_9gKQ72L4",
    "outputId": "9a21cb7f-2d9d-4b0c-a94d-8c4029611b4d"
   },
   "outputs": [],
   "source": [
    "# Import the location of modern day distilleries\n",
    "url3 = \"https://github.com/DCS-training/StatAccountScotland/blob/main/Spatial/ScottishDistilleries.gpkg?raw=true\"\n",
    "PointsDistilleries = gpd.read_file(url3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 994
    },
    "id": "TNlG208s8SZq",
    "outputId": "fbc20710-b3dc-4ec4-ead8-0420a756b7c9"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "# Plot area data\n",
    "MergedWisky.plot(\n",
    "    column='Percentage',\n",
    "    cmap=color_palette3,\n",
    "    linewidth=0.8,\n",
    "    ax=ax,\n",
    "    edgecolor='black',\n",
    "    legend=True,\n",
    "    cax=cax,\n",
    "    missing_kwds={\n",
    "        'color': 'lightgrey',\n",
    "        'label': 'No data'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Handling points with custom icons\n",
    "# Fetch and load the image from the web\n",
    "url = \"https://raw.githubusercontent.com/DCS-training/StatAccountScotland/main/Spatial/bottle.png\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = np.array(img)\n",
    "imagebox = OffsetImage(img, zoom=0.1)  # Adjust zoom as necessary\n",
    "\n",
    "for x, y in zip(PointsDistilleries.geometry.x, PointsDistilleries.geometry.y):\n",
    "    ab = AnnotationBbox(imagebox, (x, y), frameon=False, pad=0.1, box_alignment=(0.5, 0.5))\n",
    "    ax.add_artist(ab)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_title(\"Whisky Reports across Scotland\", fontsize=15)\n",
    "\n",
    "# Add scale bar\n",
    "scalebar = AnchoredSizeBar(ax.transData,\n",
    "                           100000, '100 km', 'lower left',\n",
    "                           pad=0.4,\n",
    "                           color='black',\n",
    "                           frameon=False,\n",
    "                           size_vertical=4,\n",
    "                           fontproperties=fm.FontProperties(size=12))\n",
    "ax.add_artist(scalebar)\n",
    "\n",
    "# Add North Arrow\n",
    "x, y, arrow_length = 0.95, 0.1, 0.1\n",
    "ax.annotate('N', xy=(x, y), xytext=(x, y-arrow_length),\n",
    "            arrowprops=dict(facecolor='black', width=5, headwidth=15),\n",
    "            ha='center', va='center', fontsize=12, xycoords=ax.transAxes)\n",
    "# Disable axis\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWudz2o99fPs"
   },
   "source": [
    "## Final Reflection\n",
    "Is the location of modern distilleries similar to the location of historical ones?\n",
    "Why do you think it may be?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
